/media/data_dump_1/arushi_mca/GAN_try/AttnGAN/code/miscc/config.py:103: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  yaml_cfg = edict(yaml.load(f))
/home/arushi_mca/.conda/envs/MCAProject/lib/python2.7/site-packages/torchvision/transforms/transforms.py:220: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.
  "please use transforms.Resize instead.")
/home/arushi_mca/.conda/envs/MCAProject/lib/python2.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/media/data_dump_1/arushi_mca/GAN_try/AttnGAN/code/miscc/utils.py:295: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.
  nn.init.orthogonal(m.weight.data, 1.0)
/media/data_dump_1/arushi_mca/GAN_try/AttnGAN/code/miscc/utils.py:290: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.
  nn.init.orthogonal(m.weight.data, 1.0)
Using config:
{'B_VALIDATION': False,
 'CONFIG_NAME': 'attn2',
 'CUDA': False,
 'DATASET_NAME': 'recipe',
 'DATA_DIR': '../data/recipe',
 'GAN': {'B_ATTENTION': True,
         'B_DCGAN': False,
         'CONDITION_DIM': 100,
         'DF_DIM': 96,
         'GF_DIM': 48,
         'R_NUM': 3,
         'Z_DIM': 100},
 'GPU_ID': 0,
 'RNN_TYPE': 'LSTM',
 'TEXT': {'CAPTIONS_PER_IMAGE': 1, 'EMBEDDING_DIM': 188, 'WORDS_NUM': 12052},
 'TRAIN': {'BATCH_SIZE': 8,
           'B_NET_D': False,
           'DISCRIMINATOR_LR': 0.0002,
           'ENCODER_LR': 0.0002,
           'FLAG': True,
           'GENERATOR_LR': 0.0002,
           'MAX_EPOCH': 600,
           'NET_E': '/media/data_dump_1/arushi_mca/GAN_try/AttnGAN/output/recipe_DAMSM_2020_04_26_11_12_07/Model/text_encoder50.pth',
           'NET_G': '',
           'RNN_GRAD_CLIP': 0.25,
           'SMOOTH': {'GAMMA1': 5.0,
                      'GAMMA2': 5.0,
                      'GAMMA3': 10.0,
                      'LAMBDA': 1.0},
           'SNAPSHOT_INTERVAL': 2000},
 'TREE': {'BASE_SIZE': 64, 'BRANCH_NUM': 3},
 'WORKERS': 1}
Load filenames from: ../data/recipe/train/filenames.pickle (10000)
Load filenames from: ../data/recipe/test/filenames.pickle (2000)
Load from:  ../data/recipe/captions.pickle
('Load pretrained model from ', 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth')
Load image encoder from: /media/data_dump_1/arushi_mca/GAN_try/AttnGAN/output/recipe_DAMSM_2020_04_26_11_12_07/Model/image_encoder50.pth
Load text encoder from: /media/data_dump_1/arushi_mca/GAN_try/AttnGAN/output/recipe_DAMSM_2020_04_26_11_12_07/Model/text_encoder50.pth
# of netsD 3
/home/arushi_mca/.conda/envs/MCAProject/lib/python2.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/media/data_dump_1/arushi_mca/GAN_try/AttnGAN/code/GlobalAttention.py:109: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  attn = self.sm(attn)  # Eq. (2)
/home/arushi_mca/.conda/envs/MCAProject/lib/python2.7/site-packages/torch/nn/functional.py:2506: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
/media/data_dump_1/arushi_mca/GAN_try/AttnGAN/code/GlobalAttention.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  attn = nn.Softmax()(attn)  # Eq. (8)
/media/data_dump_1/arushi_mca/GAN_try/AttnGAN/code/GlobalAttention.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  attn = nn.Softmax()(attn)
